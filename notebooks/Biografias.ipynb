{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07719f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas import json_normalize\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from rapidfuzz import process, fuzz\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aef40ef7-dd2a-4fb8-9b4c-f98e492cebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q ollama\n",
    "\n",
    "# Import and create a client that points explicitly to the local daemon\n",
    "from ollama import Client\n",
    "\n",
    "# Explicit host helps avoid accidental DNS problems\n",
    "client = Client(host=\"http://127.0.0.1:11434\")   # <- change if your server runs elsewhere\n",
    "\n",
    "# Pick a model that you have already pulled locally\n",
    "model_name = \"gpt-oss:120b-cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8903de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si es notebook, mejor usar:\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "# Ir un nivel arriba (de notebooks → raíz del repo)\n",
    "ROOT = BASE_DIR.parent\n",
    "\n",
    "# Carpeta data dentro del repo\n",
    "DATA_DIR = ROOT / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89d1a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodos = ['2022-2026'] # <- Elegir períodos\n",
    "df_par = pd.read_csv(DATA_DIR / 'parlamentarios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8d696a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def hybrid_match_nombre(nombre, lista_nombres):\n",
    "    \"\"\"\n",
    "    Busca el nombre más similar combinando token_set_ratio y partial_token_set_ratio.\n",
    "    \"\"\"\n",
    "    mejores = process.extract(\n",
    "        nombre,\n",
    "        lista_nombres,\n",
    "        scorer=fuzz.token_set_ratio,\n",
    "        limit=5\n",
    "    )\n",
    "\n",
    "    # Recalcula con partial_token_set_ratio para cada candidato\n",
    "    mejor_nombre = None\n",
    "    mejor_score = 0\n",
    "\n",
    "    for candidato, score1, _ in mejores:\n",
    "        score2 = fuzz.partial_token_set_ratio(nombre, candidato)\n",
    "        score_final = max(score1, score2)  # o podrías usar promedio: (score1 + score2) / 2\n",
    "        if score_final > mejor_score:\n",
    "            mejor_score = score_final\n",
    "            mejor_nombre = candidato\n",
    "\n",
    "    return mejor_nombre, mejor_score\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = name.strip().lower()\n",
    "    name = \" \".join(name.split())  # quita dobles espacios\n",
    "    name = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', name)\n",
    "        if unicodedata.category(c) != 'Mn'  # elimina acentos\n",
    "    )\n",
    "    return name\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip(\" \\t\\n\\r\")\n",
    "\n",
    "def fetch_html(url: str, sleep_=0.25):\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    sleep(sleep_)\n",
    "    return r.status_code, r.text\n",
    "\n",
    "def get_section_paragraphs(soup: BeautifulSoup, title_patterns):\n",
    "    \"\"\"\n",
    "    Busca div.box_contenidos con <h4> cuyo texto calce con alguno de title_patterns (regex o strings),\n",
    "    y devuelve una lista de párrafos (texto limpio) dentro de ese box.\n",
    "    \"\"\"\n",
    "    if not isinstance(title_patterns, (list, tuple)):\n",
    "        title_patterns = [title_patterns]\n",
    "\n",
    "    paras = []\n",
    "    for box in soup.select(\"div.box_contenidos\"):\n",
    "        h4 = box.find(\"h4\")\n",
    "        if not h4:\n",
    "            continue\n",
    "        title = norm(h4.get_text(\" \", strip=True))\n",
    "        if any(re.search(p, title, flags=re.I) for p in title_patterns):\n",
    "            # en tu HTML: h4 + div con <p>… Tomamos todos los <p> dentro del box.\n",
    "            for p in box.find_all(\"p\"):\n",
    "                t = norm(p.get_text(\" \", strip=True))\n",
    "                if t:\n",
    "                    paras.append(t)\n",
    "    return paras\n",
    "\n",
    "def extract_paragraphs_from_url(url: str, sleep_=0.25):\n",
    "    out = {\n",
    "        \"status\": None,\n",
    "        \"familia_juventud_parrafos\": [],\n",
    "        \"estudios_vida_laboral_parrafos\": [],\n",
    "    }\n",
    "    status, html = fetch_html(url)\n",
    "    out[\"status\"] = status\n",
    "    if status != 200:\n",
    "        return out\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    resultados = []\n",
    "    for td in soup.find_all(\"td\", class_=\"trayectoria_align\"):\n",
    "        texto_cargo = td.get_text(\" \", strip=True)\n",
    "\n",
    "        # --- 1. Extraer período robusto (acepta texto o meses en el año final) ---\n",
    "        match_periodo = re.search(r\"(\\d{4})\\s*[-–]\\s*[A-Za-z]*\\s*(\\d{4})\", texto_cargo)\n",
    "        if not match_periodo:\n",
    "            continue\n",
    "\n",
    "        periodo = f\"{match_periodo.group(1)}-{match_periodo.group(2)}\"\n",
    "        if periodo not in periodos:\n",
    "            continue\n",
    "\n",
    "        # --- 2. Extraer distrito ---\n",
    "        distrito_num = None\n",
    "\n",
    "        # Caso A: property específico\n",
    "        span_distrito = td.find(\"span\", {\"property\": \"bcnbio:representingPlaceNamed\"})\n",
    "        if span_distrito:\n",
    "            texto = span_distrito.get_text(\" \", strip=True)\n",
    "            m = re.search(r\"(\\d+)\", texto)\n",
    "            if m:\n",
    "                distrito_num = int(m.group(1))\n",
    "\n",
    "        # Caso B: texto genérico \"Distrito\"\n",
    "        if distrito_num is None:\n",
    "            for div in td.find_all(\"div\"):\n",
    "                texto = div.get_text(\" \", strip=True)\n",
    "                if \"Distrito\" in texto:\n",
    "                    m = re.search(r\"(\\d+)\", texto)\n",
    "                    if m:\n",
    "                        distrito_num = int(m.group(1))\n",
    "                    break\n",
    "    try:\n",
    "        out[\"distrito\"] = distrito_num\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # Captura exacta de tus secciones\n",
    "    fam = get_section_paragraphs(soup, [r\"Familia\\s+y\\s+Juventud\", r\"Familia\", r\"Juventud\"])\n",
    "    est = get_section_paragraphs(soup, [r\"Estudios\\s+y\\s+vida\\s+laboral\", r\"Estudios\", r\"Vida\\s+laboral\"])\n",
    "    \n",
    "    out[\"familia_juventud_parrafos\"] = fam\n",
    "    out[\"estudios_vida_laboral_parrafos\"] = est\n",
    "    \n",
    "    return out\n",
    "\n",
    "def extraer_info(biografia):\n",
    "    \n",
    "    prompt = f\"\"\"Extrae la siguiente información biográfica desde el texto que te entregaré a continuación.\n",
    "Si algún dato no está presente, responde con \"desconocido\" o deja el campo vacío.\n",
    "Entrega la respuesta en formato JSON válido con los siguientes campos:\n",
    "\n",
    "- lugar_nacimiento\n",
    "- fecha_nacimiento\n",
    "- padre\n",
    "- madre\n",
    "- estado_civil\n",
    "- numero_total_hijos (número)\n",
    "- colegios (Lista de colegios en orden)\n",
    "- universidad (Solo si terminó la carrera)\n",
    "- carrera (Solo si terminó la carrera)\n",
    "- maximo_nivel_educativo (Enseñanza Básica, Enseñanza Media, Educación Universitaria, Magíster o Doctor/a)\n",
    "- trabajo (lista trabajos)\n",
    "\n",
    "Texto:\n",
    "{biografia}\n",
    "Responde solo con el JSON estructurado.\n",
    "    \"\"\"\n",
    "    # Build the message list\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    # Call the API (stream=False for a quick test, then you can enable streaming)\n",
    "    response = client.chat(model_name, messages=messages, stream=False)\n",
    "    json_str = response[\"message\"][\"content\"].replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    try:\n",
    "        datos = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        datos = {}\n",
    "        \n",
    "    for k, v in list(datos.items()):\n",
    "        if isinstance(v, str) and v.strip().lower() == \"desconocido\":\n",
    "            datos[k] = None\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7185db9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coincidencias fuertes: 157 de 157 (100.0%)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'biografia_completa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'biografia_completa'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 48\u001b[0m\n\u001b[0;32m     42\u001b[0m df_check \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_dip\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), df_parrafos], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m df_check[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiografía_completa\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     44\u001b[0m df_check[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfamilia_juventud_parrafos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     45\u001b[0m df_check[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestudios_vida_laboral_parrafos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x))\n\u001b[0;32m     46\u001b[0m )\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 48\u001b[0m df_extraido \u001b[38;5;241m=\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiografia_completa\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extraer_info)\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mSeries)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Unimos al DataFrame original\u001b[39;00m\n\u001b[0;32m     51\u001b[0m df_bio \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_bio, df_extraido], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'biografia_completa'"
     ]
    }
   ],
   "source": [
    "for periodo in periodos:\n",
    "\n",
    "    df_dip = pd.read_csv(DATA_DIR / periodo / 'diputados.csv')\n",
    "    df_dip[\"nombre_norm\"] = df_dip[\"nombre_completo\"].apply(normalize_name)\n",
    "    df_par[\"nombre_norm\"] = df_par[\"nombre_en_lista\"].apply(normalize_name)\n",
    "    \n",
    "    df_dip = df_dip.drop_duplicates(subset=[\"nombre_norm\"]).reset_index(drop=True)\n",
    "    \n",
    "    resultados = []\n",
    "    for nombre in df_dip['nombre_norm']:\n",
    "        nombre_match, score = hybrid_match_nombre(nombre, df_par[\"nombre_norm\"])\n",
    "        resultados.append((nombre, nombre_match, score))\n",
    "\n",
    "    # Crear DataFrame auxiliar con los resultados únicos\n",
    "    df_match_unique = pd.DataFrame(resultados, columns=[\"nombre_norm\", \"nombre_match_norm\", \"score\"])\n",
    "\n",
    "    # Merge de vuelta al DataFrame original (respetando duplicados en df_dip)\n",
    "    df_dip = df_dip.merge(df_match_unique, on=\"nombre_norm\", how=\"left\")\n",
    "\n",
    "    # Combinar con ex parlamentarios para traer nombre y URL\n",
    "    df_final = df_dip.merge(\n",
    "        df_par,\n",
    "        left_on=\"nombre_match_norm\",\n",
    "        right_on=\"nombre_norm\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    df_final = (\n",
    "    df_final.sort_values(\"score\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"nombre_completo\"], keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Reporte rápido\n",
    "    n_match = (df_final[\"score\"] >= 70).sum()\n",
    "    total = len(df_final)\n",
    "    print(f\"Coincidencias fuertes: {n_match} de {total} ({n_match/total:.1%})\")\n",
    "\n",
    "    # Extracción de información biográfica\n",
    "    res = df_final['url_wiki'].apply(extract_paragraphs_from_url)\n",
    "    df_parrafos = pd.DataFrame(list(res))\n",
    "    df_check = pd.concat([df_dip.reset_index(drop=True), df_parrafos], axis=1)\n",
    "    \n",
    "    df_check['biografia_completa'] = (\n",
    "    df_check['familia_juventud_parrafos'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)) + ' ' +\n",
    "    df_check['estudios_vida_laboral_parrafos'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "    ).str.strip()\n",
    "\n",
    "    df_extraido = df_check[\"biografia_completa\"].apply(extraer_info).apply(pd.Series)\n",
    "    # Unimos al DataFrame original\n",
    "    df_bio = pd.concat([df_check, df_extraido], axis=1)\n",
    "    df_bio.to_csv(DATA_DIR / periodo / 'diputados_bio.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fd4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
